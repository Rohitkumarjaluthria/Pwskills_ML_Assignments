{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfc7c72b",
   "metadata": {},
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values.\n",
    "\n",
    "Missing values in a dataset refer to the absence of data for certain variables or observations. Handling missing values is essential for several reasons:\n",
    "\n",
    "Missing values can lead to biased or misleading analysis if ignored, as they can affect statistical measures and model performance.\n",
    "\n",
    "Missing values can result in reduced sample size and loss of valuable information.\n",
    "\n",
    "Ignoring missing values can lead to biased parameter estimates and inaccurate results.\n",
    "\n",
    "Some algorithms that are not affected by missing values include:\n",
    "\n",
    "Tree-based algorithms such as Random Forest and Gradient Boosting Machines (GBM): These algorithms can handle missing values by treating missing values as a separate category during the splitting process.\n",
    "\n",
    "Support Vector Machines (SVM) with appropriate kernels: SVMs can work well with missing values if the kernel function can handle them.\n",
    "\n",
    "Q2: List down techniques used to handle missing data. Give an example of each with python code.\n",
    "\n",
    " Techniques used to handle missing data include:\n",
    "\n",
    "Deletion: Dropping observations or variables with missing values.\n",
    "\n",
    "Mean/Median imputation: Replacing missing values with the mean or median of the available values.\n",
    "\n",
    "Mode imputation: Replacing missing categorical values with the mode (most frequent value) of the available values.\n",
    "\n",
    "Regression imputation: Predicting missing values using regression models based on other variables.\n",
    "\n",
    "Multiple imputation: Creating multiple imputations by generating plausible values for missing data.\n",
    "\n",
    "python :-\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df['column_name'].fillna(df['column_name'].mean(), inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "\n",
    "Imbalanced data refers to a dataset where the distribution of classes is skewed, with one class being significantly more prevalent than the others. If imbalanced data is not handled properly, it can lead to biased models and poor predictions. In classification tasks, the model tends to favor the majority class, resulting in low accuracy for the minority class. This can have serious consequences, especially in applications where correctly identifying the minority class is crucial, such as fraud detection or disease diagnosis.\n",
    "\n",
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required.\n",
    "\n",
    "Up-sampling and down-sampling are techniques used to address class imbalance in a dataset.\n",
    "\n",
    "Up-sampling: It involves randomly duplicating instances from the minority class to increase its representation in the dataset. This is done to balance the class distribution and provide more training data for the minority class.\n",
    "\n",
    "Down-sampling: It involves randomly selecting a subset of instances from the majority class to reduce its representation in the dataset. This is done to balance the class distribution by reducing the dominance of the majority class.\n",
    "\n",
    "The choice between up-sampling and down-sampling depends on the specific problem and the available data. Up-sampling is typically used when the minority class is underrepresented and down-sampling when the majority class is overrepresented.\n",
    "\n",
    "Q5: What is data Augmentation? Explain SMOTE.\n",
    "\n",
    "Data augmentation is a technique used to artificially increase the size of a dataset by applying various transformations or modifications to the existing data. It is commonly used in image and text data. SMOTE (Synthetic Minority Over-sampling Technique) is a specific data augmentation method used to address class imbalance. SMOTE generates synthetic samples for the minority class by creating new instances along the line segments connecting existing minority class instances. This helps to balance the class distribution and provide more diverse training data for the minority class.\n",
    "\n",
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "\n",
    "Outliers in a dataset are observations that deviate significantly from the majority of other observations. They can occur due to errors, measurement issues, or natural variations in the data. It is essential to handle outliers because they can have a disproportionate impact on statistical measures and model performance. Outliers can distort data distribution, affect the estimation of parameters, and lead to biased results. Handling outliers involves techniques such as removal, transformation, or using robust statistical measures to mitigate their influence and improve the accuracy and robustness of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bb9373",
   "metadata": {},
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "\n",
    "Some techniques to handle missing data in your analysis include:\n",
    "\n",
    "Deletion: You can delete the rows or columns with missing data if the missingness is minimal and doesn't significantly affect the analysis.\n",
    "Mean/Median imputation: You can replace missing values with the mean or median of the available values for numerical variables.\n",
    "\n",
    "Mode imputation: For categorical variables, you can replace missing values with the mode (most frequent value) of the available values.\n",
    "\n",
    "Regression imputation: You can predict missing values using regression models based on other variables in the dataset.\n",
    "\n",
    "Multiple imputation: You can generate multiple imputations by creating plausible values for missing data based on statistical methods.\n",
    "\n",
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?\n",
    "\n",
    "Visual inspection: Plotting the missing data patterns or using heatmaps can help identify if there are any observable patterns or correlations in missing values.\n",
    "\n",
    "Statistical tests: Conducting statistical tests to examine if there are any systematic relationships between missingness and other variables can provide insights into the missing data mechanism.\n",
    "\n",
    "Missingness indicators: Creating indicators or flags to represent the missingness in variables can help detect patterns or dependencies in the missing data.\n",
    "\n",
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "\n",
    "Strategies to evaluate the performance of a machine learning model on an imbalanced dataset with a majority class and a small percentage of the minority class include:\n",
    "\n",
    "Precision and recall: Instead of relying solely on accuracy, consider precision (true positives divided by predicted positives) and recall (true positives divided by actual positives) metrics, as they provide more meaningful insights for imbalanced datasets.\n",
    "\n",
    "F1-score: Use the F1-score, which is the harmonic mean of precision and recall, as an overall evaluation metric that considers both precision and recall.\n",
    "\n",
    "ROC curve and AUC: Plotting the receiver operating characteristic (ROC) curve and calculating the area under the curve (AUC) can help assess the model's ability to discriminate between classes.\n",
    "\n",
    "Resampling techniques: Apply resampling techniques like up-sampling the minority class, down-sampling the majority class, or using more advanced techniques like SMOTE or ADASYN to balance the class distribution and improve model performance.\n",
    "\n",
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?\n",
    "\n",
    "To balance an imbalanced dataset with the majority class, you can employ down-sampling techniques such as:\n",
    "\n",
    "Random under-sampling: Randomly selecting a subset of instances from the majority class to match the size of the minority class.\n",
    "\n",
    "Cluster-based under-sampling: Identifying clusters within the majority class and selecting representative instances from each cluster to down-sample.\n",
    "\n",
    "Tomek links: Identifying pairs of instances from different classes that are closest to each other and removing the majority class instances.\n",
    "\n",
    "NearMiss: Selecting majority class instances based on their distances to the minority class instances.\n",
    "\n",
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?\n",
    "\n",
    "To balance an imbalanced dataset with a low percentage of occurrences in the minority class, you can employ up-sampling techniques such as:\n",
    "\n",
    "Random over-sampling: Randomly duplicating instances from the minority class to increase its representation in the dataset.\n",
    "\n",
    "SMOTE: Synthetic Minority Over-sampling Technique generates synthetic samples for the minority class by creating new instances along the line segments connecting existing minority class instances.\n",
    "\n",
    "ADASYN: Adaptive Synthetic Sampling focuses on generating synthetic samples for the minority class based on the density distribution of the data.\n",
    "\n",
    "SMOTE-ENN: Combining SMOTE with Edited Nearest Neighbors, where SMOTE is first used to over-sample the minority class, followed by removing noisy instances using the Edited Nearest Neighbors algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c6791b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
